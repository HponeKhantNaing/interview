[
  {"question": "What is the difference between an array and a linked list?","answer": "Arrays store elements in contiguous memory locations with direct access via index, while linked lists store elements in nodes with pointers to the next node. Arrays have O(1) access time but O(n) insertion/deletion, while linked lists have O(n) access but O(1) insertion/deletion at head/tail.","topics": ["arrays", "linked lists", "data structures", "time complexity"]},
  {"question": "Explain the concept of stacks and queues.","answer": "Stacks follow LIFO (Last In, First Out) principle - elements are added and removed from the same end. Queues follow FIFO (First In, First Out) principle - elements are added at one end and removed from the other. Both are linear data structures with specific use cases.","topics": ["stacks", "queues", "lifo", "fifo", "data structures"]},
  {"question": "What is a binary search tree and what are its properties?","answer": "A binary search tree is a hierarchical data structure where each node has at most two children. For any node, all values in the left subtree are less than the node's value, and all values in the right subtree are greater. This property enables efficient searching with O(log n) average time complexity.","topics": ["binary search tree", "bst", "trees", "data structures"]},
  {"question": "Explain the difference between DFS and BFS.","answer": "DFS (Depth-First Search) explores as far as possible along each branch before backtracking, using a stack or recursion. BFS (Breadth-First Search) explores all neighbors at the current depth before moving to the next level, using a queue. DFS is memory-efficient for deep trees, while BFS finds shortest paths in unweighted graphs.","topics": ["dfs", "bfs", "graph traversal", "algorithms"]},
  {"question": "How does QuickSort work and what is its time complexity?","answer": "QuickSort uses a divide-and-conquer approach by selecting a pivot element and partitioning the array around it. Elements smaller than pivot go to the left, larger to the right. It has O(n log n) average time complexity but O(n²) worst case. Space complexity is O(log n) due to recursion stack.","topics": ["quicksort", "sorting", "divide and conquer", "algorithms"]},
  {"question": "Explain binary search and its requirements.","answer": "Binary search is an efficient algorithm for finding elements in a sorted array. It repeatedly divides the search interval in half, comparing the target with the middle element. It has O(log n) time complexity but requires the array to be sorted.","topics": ["binary search", "searching", "algorithms", "time complexity"]},
  {"question": "What is Big O notation and why is it important?","answer": "Big O notation describes the upper bound of an algorithm's time or space complexity as input size grows. It helps compare algorithm efficiency and predict performance. Common complexities include O(1), O(log n), O(n), O(n log n), O(n²), and O(2ⁿ).","topics": ["big o notation", "time complexity", "space complexity", "algorithm analysis"]},
  {"question": "Explain the concept of dynamic programming.","answer": "Dynamic programming solves complex problems by breaking them into simpler subproblems and storing solutions to avoid redundant calculations. It uses memoization (top-down) or tabulation (bottom-up) approaches. Common applications include Fibonacci, longest common subsequence, and knapsack problems.","topics": ["dynamic programming", "memoization", "tabulation", "algorithms"]},
  {"question": "What is recursion and when should you use it?","answer": "Recursion is when a function calls itself to solve smaller instances of the same problem. It's useful for problems with recursive structure like tree traversal, factorial calculation, and divide-and-conquer algorithms. Base cases are crucial to prevent infinite recursion.","topics": ["recursion", "base case", "algorithms", "programming"]},
  {"question": "Explain the concept of backtracking.","answer": "Backtracking is a systematic way to search for solutions by building candidates incrementally and abandoning candidates that fail constraints. It's used for problems like N-queens, Sudoku, and generating permutations. When a path fails, it backtracks to try alternatives.","topics": ["backtracking", "constraint satisfaction", "algorithms"]},
  {"question": "What is a hash table and how does it achieve O(1) average access?","answer": "A hash table stores key-value pairs using a hash function to map keys to array indices. It achieves O(1) average access by distributing elements evenly across the array. Collisions are handled by chaining (linked lists) or open addressing (probing).","topics": ["hash table", "hashmap", "hashing", "collision resolution"]},
  {"question": "Explain the difference between a stack and a heap in memory management.","answer": "The stack is a LIFO data structure for function calls and local variables, managed automatically by the compiler. The heap is a dynamic memory area for objects allocated at runtime, requiring manual management in some languages. Stack is faster but limited in size.","topics": ["stack", "heap", "memory management", "data structures"]},
  {"question": "What is the difference between a tree and a graph?","answer": "A tree is a connected acyclic graph with exactly one path between any two nodes. A graph can have cycles and multiple paths between nodes. Trees have a hierarchical structure with a root node, while graphs can have any structure.","topics": ["trees", "graphs", "data structures", "hierarchical"]},
  {"question": "Explain the concept of time-space tradeoff.","answer": "Time-space tradeoff is the relationship between an algorithm's time complexity and space complexity. Often, you can reduce time complexity by using more space (caching, memoization) or reduce space complexity by using more time (recomputing values).","topics": ["time complexity", "space complexity", "tradeoff", "algorithm design"]},
  {"question": "What is the difference between a min-heap and a max-heap?","answer": "A min-heap is a complete binary tree where each parent node is smaller than or equal to its children, with the minimum element at the root. A max-heap has each parent larger than or equal to its children, with the maximum at the root. Both support O(log n) insertion and deletion.","topics": ["heap", "min-heap", "max-heap", "priority queue"]},
  {"question": "Explain the concept of amortized analysis.","answer": "Amortized analysis provides average performance over a sequence of operations, even if individual operations are expensive. It's used for data structures like dynamic arrays and hash tables where occasional expensive operations are balanced by many cheap operations.","topics": ["amortized analysis", "algorithm analysis", "data structures"]},
  {"question": "What is the difference between a queue and a priority queue?","answer": "A queue follows FIFO order, while a priority queue orders elements by priority (typically implemented as a heap). Elements with higher priority are dequeued first, regardless of insertion order. Priority queues are used in algorithms like Dijkstra's shortest path.","topics": ["queue", "priority queue", "heap", "data structures"]},
  {"question": "Explain the concept of graph algorithms: Dijkstra's vs Bellman-Ford.","answer": "Dijkstra's algorithm finds shortest paths in graphs with non-negative edge weights using a priority queue, with O((V + E) log V) time complexity. Bellman-Ford handles negative weights but has O(VE) complexity and can detect negative cycles.","topics": ["dijkstra", "bellman-ford", "shortest path", "graph algorithms"]},
  {"question": "What is the difference between merge sort and quick sort?","answer": "Merge sort is a stable, divide-and-conquer algorithm with guaranteed O(n log n) time complexity but requires O(n) extra space. Quick sort is in-place but unstable, with O(n log n) average case but O(n²) worst case. Merge sort is better for linked lists.","topics": ["merge sort", "quick sort", "sorting", "algorithms"]},
  {"question": "Explain the concept of string matching algorithms.","answer": "String matching finds patterns within text. Naive approach has O(mn) complexity. KMP (Knuth-Morris-Pratt) preprocesses the pattern for O(m + n) complexity. Boyer-Moore uses right-to-left scanning for better average performance.","topics": ["string matching", "kmp", "boyer-moore", "algorithms"]}
] 